{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29574d5-799a-4ead-a7a9-b5f08a322b97",
   "metadata": {},
   "source": [
    "\n",
    "Separating SMOTE and ADASYN evaluations into a dedicated, follow-up notebook allows for a more controlled and focused analysis of each resampling technique's performance. By isolating this comparison, we can systematically assess their respective impacts on model training, validation, and prediction outcomes without interference from other preprocessing steps or analyses. This approach enables a clearer comparison of metrics, like ROC AUC and classification accuracy, and ensures that the results are interpretable and actionable. Additionally, having a standalone notebook makes it easier to share insights and replicate results for future projects involving imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3060baed-6d72-4a8e-b383-bf7a4904f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fa67e8-0158-42d7-9981-accd21347675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb723b24-7e7f-41b2-a620-8e9840aa8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"id\", axis=1, inplace=True) #we don't want to include this label when training our model\n",
    "X = df.drop('loan_status', axis=1)\n",
    "y = df['loan_status'] #label we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467dd158-70ac-4539-9c01-0db3c6b6fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\n",
    "categorical_features = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']\n",
    "\n",
    "# Define ColumnTransformer for scaling and one-hot encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edcd899f-c8bb-447a-a24b-afaa6c05b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a67b910-85a3-43e2-a9fb-34ee94945c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b26e9a5-852a-4841-80a5-10cfbc84ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset distribution: Counter({0: 40216, 1: 6700})\n",
      "SMOTE oversampled dataset distribution: Counter({0: 40216, 1: 40216})\n",
      "ADASYN oversampled dataset distribution: Counter({1: 40547, 0: 40216})\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "adasyn = ADASYN(sampling_strategy='minority',random_state=42)\n",
    "print(\"Original dataset distribution:\", Counter(y_train))\n",
    "\n",
    "# Apply SMOTE on the training dataset\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Apply ADASYN on the training dataset\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new distribution after SMOTE and ADASYN\n",
    "print(\"SMOTE oversampled dataset distribution:\", Counter(y_train_smote))\n",
    "print(\"ADASYN oversampled dataset distribution:\", Counter(y_train_adasyn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "955b8a24-21d7-4fd5-bfff-bcee3bb27ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_smote = xgb.XGBClassifier(\n",
    "    max_depth=8,              # Max depth of trees\n",
    "    learning_rate=0.005,       # Learning rate (step size shrinkage)\n",
    "    n_estimators=8000,         # Number of trees to be built\n",
    "    subsample=0.8,            # Fraction of samples used per tree\n",
    "    colsample_bytree=1,     # Fraction of features used per tree\n",
    "    colsample_bylevel=0.8,    # Fraction of features per tree level\n",
    "    min_child_weight=1,       # Minimum sum of instance weight in a child\n",
    "    gamma=0.005,                # Minimum loss reduction required for split\n",
    "    scale_pos_weight=1,       # Balancing positive/negative classes\n",
    "    reg_alpha=0.4,           # L1 regularization\n",
    "    reg_lambda=0.15,           # L2 regularization\n",
    "    tree_method='hist',       # Use histogram-based algorithm\n",
    "    random_state=42,          # Seed for reproducibility\n",
    "    objective='binary:logistic',  # For binary classification\n",
    "    eval_metric='auc',        # Evaluation metric\n",
    "    n_jobs=-1                 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "108a5473-4625-4665-8b43-674457ef4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_adasyn = xgb.XGBClassifier(\n",
    "    max_depth=8,              # Max depth of trees\n",
    "    learning_rate=0.005,       # Learning rate (step size shrinkage)\n",
    "    n_estimators=8000,         # Number of trees to be built\n",
    "    subsample=0.8,            # Fraction of samples used per tree\n",
    "    colsample_bytree=1,     # Fraction of features used per tree\n",
    "    colsample_bylevel=0.8,    # Fraction of features per tree level\n",
    "    min_child_weight=1,       # Minimum sum of instance weight in a child\n",
    "    gamma=0.005,                # Minimum loss reduction required for split\n",
    "    scale_pos_weight=1,       # Balancing positive/negative classes\n",
    "    reg_alpha=0.4,           # L1 regularization\n",
    "    reg_lambda=0.15,           # L2 regularization\n",
    "    tree_method='hist',       # Use histogram-based algorithm\n",
    "    random_state=42,          # Seed for reproducibility\n",
    "    objective='binary:logistic',  # For binary classification\n",
    "    eval_metric='auc',        # Evaluation metric\n",
    "    n_jobs=-1                 # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b344fb-97a6-4af8-b383-486663241463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROC AUC with SMOTE: 0.9404447268681139\n",
      "Average ROC AUC with ADASYN: 0.9404447268681139\n"
     ]
    }
   ],
   "source": [
    "# SMOTE pipeline\n",
    "xgb_model_smote.fit(X_train_smote, y_train_smote)\n",
    "smote_scores = cross_val_score(xgb_model_smote, X_cv, y_cv, scoring='roc_auc', cv=5)\n",
    "\n",
    "# ADASYN pipeline\n",
    "xgb_model_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
    "adasyn_scores = cross_val_score(xgb_model_adasyn, X_cv, y_cv, scoring='roc_auc', cv=5)\n",
    "\n",
    "# Compare scores\n",
    "print(\"Average ROC AUC with SMOTE:\", smote_scores.mean())\n",
    "print(\"Average ROC AUC with ADASYN:\", adasyn_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fc41070-1447-4112-88a3-fe11b84cd0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROC AUC with SMOTE across folds: 0.9912255596174189\n",
      "Average ROC AUC with ADASYN across folds: 0.9906954358506368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to hold the AUC scores for each method\n",
    "smote_aucs = []\n",
    "adasyn_aucs = []\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "for train_index, val_index in skf.split(X, y):\n",
    "    # Split data\n",
    "    X_train, X_cv = X[train_index], X[val_index]\n",
    "    y_train, y_cv = y[train_index], y[val_index]\n",
    "    \n",
    "    # SMOTE Pipeline\n",
    "    xgb_model_smote.fit(X_train_smote, y_train_smote)\n",
    "    y_pred_smote = xgb_model_smote.predict_proba(X_cv)[:, 1]\n",
    "    smote_auc = roc_auc_score(y_cv, y_pred_smote)\n",
    "    smote_aucs.append(smote_auc)\n",
    "    \n",
    "    # ADASYN Pipeline\n",
    "    xgb_model_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
    "    y_pred_adasyn = xgb_model_adasyn.predict_proba(X_cv)[:, 1]\n",
    "    adasyn_auc = roc_auc_score(y_cv, y_pred_adasyn)\n",
    "    adasyn_aucs.append(adasyn_auc)\n",
    "\n",
    "# Calculate the mean AUC scores for each method\n",
    "print(\"Average ROC AUC with SMOTE across folds:\", np.mean(smote_aucs))\n",
    "print(\"Average ROC AUC with ADASYN across folds:\", np.mean(adasyn_aucs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c4c4be0-03f0-43ad-b2f1-4051350a2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'id' column is present in the test data, and we want to retain it in the output\n",
    "ids = test['id']\n",
    "# Drop the 'id' column before preprocessing and prediction\n",
    "X_new_test = test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bb13d8b-223a-4ac2-aa72-e4255d292a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new test data (same preprocessor used for training data)\n",
    "X_new_test_preprocessed = preprocessor.transform(X_new_test)\n",
    "\n",
    "# Make predictions\n",
    "predictions_smote = xgb_model_smote.predict(X_new_test_preprocessed)\n",
    "predictions_adasyn = xgb_model_adasyn.predict(X_new_test_preprocessed)\n",
    "predictions_probs_smote = xgb_model_smote.predict_proba(X_new_test_preprocessed)[:,1]\n",
    "predictions_probs_adasyn = xgb_model_adasyn.predict_proba(X_new_test_preprocessed)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b84615e-c4bb-402b-9ddf-8ec8f76206f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate test dataset distribution with SMOTE (based on predictions): Counter({0: 34700, 1: 4398})\n",
      "Approximate test dataset distribution with ADASYN (based on predictions): Counter({0: 34713, 1: 4385})\n"
     ]
    }
   ],
   "source": [
    "# Assign labels based on a threshold of 0.5\n",
    "predicted_labels_smote = [1 if prob >= 0.5 else 0 for prob in predictions_probs_smote]\n",
    "predicted_labels_adasyn = [1 if prob >= 0.5 else 0 for prob in predictions_probs_adasyn]\n",
    "\n",
    "# Count the distribution of predicted labels\n",
    "test_distribution_smote = Counter(predicted_labels_smote)\n",
    "test_distribution_adasyn = Counter(predicted_labels_adasyn)\n",
    "print(f\"Approximate test dataset distribution with SMOTE (based on predictions): {test_distribution_smote}\")\n",
    "print(f\"Approximate test dataset distribution with ADASYN (based on predictions): {test_distribution_adasyn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29dfa7bc-c261-401b-a15c-a444b154d5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01846984847130902, 0.8918973150483489)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from collections import Counter\n",
    "\n",
    "# Define the distributions based on predictions\n",
    "smote_distribution = Counter({0: 34700, 1: 4398})\n",
    "adasyn_distribution = Counter({0: 34713, 1: 4385})\n",
    "\n",
    "# Convert distributions to list format for the chi-squared test\n",
    "observed = [smote_distribution[0], smote_distribution[1]]\n",
    "expected = [adasyn_distribution[0], adasyn_distribution[1]]\n",
    "\n",
    "# Perform chi-squared test\n",
    "chi2, p_value = chi2_contingency([observed, expected])[:2]\n",
    "chi2, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e566f-814c-4564-ba55-6598651c1cc5",
   "metadata": {},
   "source": [
    "The chi-squared test comparing the SMOTE and ADASYN distributions yields a chi-squared statistic of approximately 0.018 and a p-value of 0.892. This high p-value indicates no statistically significant difference between the distributions produced by SMOTE and ADASYN in terms of class proportions in the predictions. Therefore, the sampling method does not appear to significantly alter the distribution of predicted classes in this case.\n",
    "\n",
    "This suggests that either method could be suitable, with a focus on other metrics (such as model performance) for further refinement in this context. ​​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d96d9e-1f1f-4835-9fbf-dacff6f65e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
